ğŸš€ HTTPBin APIs Performance Test Suite (JMeter + Docker Compose)
ğŸ“Œ Overview

This project evaluates the performance and stability of the HTTPBin API hosted by Postman Labs.
It measures key performance indicators like response time, throughput, and error rate under various concurrent user loads.

ğŸ§© Test Scope
HTTP Method	Endpoint	Description
GET	/get	Retrieve data
POST	/post	Submit data
PUT	/put	Update data
PATCH	/patch	Modify data
DELETE	/delete	Remove data

Objective: Validate performance KPIs under different load levels.

âš™ï¸ Performance KPIs
KPI	Description	Target
Avg Response Time	Mean time to serve requests	â‰¤ 1 sec
90th Percentile	High-end latency boundary	â‰¤ 480 ms
95th Percentile	High-end latency boundary	â‰¤ 620 ms
Throughput	Requests/sec	â‰¥ 100 req/s
Error Rate	Failed requests	â‰¤ 1%
CPU Utilization	Processing load	â‰¤ 75%
Memory Utilization	RAM usage	â‰¤ 70%
ğŸ§± Assumptions

Stable server environment with no background load

Consistent network conditions

Independent user requests (no caching)

Sufficient test duration for steady-state results

ğŸ§ª JMeter Test Structure
Thread Group (10, 100, 200, 500 users)
â”œâ”€â”€ HTTP Requests (/get, /post, /put, /patch, /delete)
â”œâ”€â”€ User Defined Variables
â”œâ”€â”€ JSR223 Assertions (Response Time, Error Rate)
â”œâ”€â”€ Listeners:
â”‚   â”œâ”€â”€ Summary & Aggregate Reports
â”‚   â”œâ”€â”€ View Results Tree/Table
â”‚   â””â”€â”€ Simple Data Writer (results.jtl)


HTML reports are generated automatically for quick analysis.

ğŸ“‚ Project Structure
â”œâ”€â”€ docker-compose.yml          # Service setup
â”œâ”€â”€ httpbin_PTTest              # JMeter Test Plan
â”œâ”€â”€ results/                    # Raw JTL outputs
â”œâ”€â”€ report-html/                # HTML reports
â””â”€â”€ README.md                   # Documentation

ğŸ³ Docker Setup

Requirements: Docker Desktop v4.48+, Docker Compose

Commands:

docker pull kennethreitz/httpbin
docker run -d --name=httpbin -p 80:80 kennethreitz/httpbin


Verify: http://localhost/get

ğŸ“Š cAdvisor Setup (Monitoring)
docker run -d --name=cadvisor \
  -v /:/rootfs:ro -v /var/run:/var/run:ro \
  -v /sys:/sys:ro -v /var/lib/docker/:/var/lib/docker:ro \
  -p 8081:8080 gcr.io/cadvisor/cadvisor:latest


Access Dashboard: http://localhost:8081

Metrics: CPU, Memory, Network I/O, Filesystem I/O
Export: http://localhost:8081/metrics > cadvisor_metrics.txt

ğŸ§¾ Test Execution
jmeter -n -t /tests/httpbin_test.jmx \
       -l /results/httpbin_results.jtl \
       -e -o /results/httpbin_report

âš™ï¸ Resource Optimization (WSL Config)
[wsl2]
memory=6GB
processors=8
swap=1GB
localhostForwarding=true

ğŸ“ˆ Performance Testing Approach
Phase	Description
Requirement Analysis	Identify NFRs, SLAs, key transactions
Workload Modeling	Define users, duration, ramp-up/down
Scenario Design	Load (10â€“500 users), Stress, Endurance, Spike tests
Monitoring	Track CPU, Memory, Throughput, Errors
Reporting	Generate HTML Dashboard and analyze KPIs
ğŸ“ Results

Raw Results: results/results.jtl

HTML Report: report-html/index.html

ğŸ§° Tools Used

Apache JMeter â€“ Performance testing

Docker Compose â€“ Container orchestration

HTTPBin â€“ REST API endpoints

cAdvisor â€“ Container monitoring

âœ… Next Steps

Integrate with CI/CD (Jenkins)

Add SLA-based assertions in JMeter

Extend test coverage to advanced endpoints
